{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e51c252f-8f5d-4255-9c1e-a098c6cf7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setups\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from sagemaker import get_execution_role, Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41681a3d-f2b4-4d36-b5ca-6deb0b995122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-472925017889/test/pytorch-training-2023-02-19-18-16-23-445/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "sess = Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "%store -r pt_mnist_model_data\n",
    "\n",
    "# Load in trained model from s3 bucket\n",
    "try:\n",
    "    pt_mnist_model_data\n",
    "except NameError:\n",
    "    import json\n",
    "\n",
    "    # copy a pretrained model from a public public to your default bucket\n",
    "    s3 = boto3.client(\"s3\")\n",
    "    bucket = CONFIG[\"public_bucket\"]\n",
    "    key = \"test/pytorch-training-2023-02-19-18-16-23-445/model.tar.gz\"\n",
    "    s3.download_file(bucket, key, \"model.tar.gz\")\n",
    "\n",
    "    # upload to default bucket\n",
    "    pt_mnist_model_data = sess.upload_data(\n",
    "        path=\"model.tar.gz\", bucket=sess.default_bucket(), key_prefix=\"model/pytorch\"\n",
    "    )\n",
    "print(pt_mnist_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "840c8848-6045-4675-bb66-ddb002dd843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PyTorchModel(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"code\",\n",
    "    role=role,\n",
    "    model_data=pt_mnist_model_data,\n",
    "    framework_version=\"1.5.0\",\n",
    "    py_version=\"py3\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c0da6ff-005e-49e5-8c49-e4208123b5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Attaching to crhfmegqhy-algo-1-hwq1z\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:29,968 [INFO ] main com.amazonaws.ml.mms.ModelServer - \n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m MMS Home: /opt/conda/lib/python3.6/site-packages\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Current directory: /\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Temp directory: /home/model-server/tmp\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Number of GPUs: 0\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Number of CPUs: 2\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Max heap size: 859 M\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Python executable: /opt/conda/bin/python3.6\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Config file: /etc/sagemaker-mms.properties\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Inference address: http://0.0.0.0:8080\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Management address: http://0.0.0.0:8080\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Model Store: /.sagemaker/mms/models\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Initial Models: ALL\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Log dir: /logs\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Metrics dir: /logs\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Netty threads: 0\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Netty client threads: 0\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Default workers per model: 2\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Blacklist Regex: N/A\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Maximum Response Size: 6553500\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Maximum Request Size: 6553500\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Preload model: false\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Prefer direct buffer: false\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:30,121 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-9000-model\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:30,272 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - model_service_worker started with args: --sock-type unix --sock-name /home/model-server/tmp/.mms.sock.9000 --handler sagemaker_pytorch_serving_container.handler_service --model-path /.sagemaker/mms/models/model --model-name model --preload-model false --tmp-dir /home/model-server/tmp\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:30,274 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Listening on port: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:30,275 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - [PID] 28\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:30,275 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - MMS worker started.\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:30,276 [INFO ] main com.amazonaws.ml.mms.wlm.ModelManager - Model model loaded.\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:30,277 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Python runtime: 3.6.13\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:30,285 [INFO ] main com.amazonaws.ml.mms.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:30,299 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:30,300 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Connecting to: /home/model-server/tmp/.mms.sock.9000\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:30,393 [INFO ] main com.amazonaws.ml.mms.ModelServer - Inference API bind to: http://0.0.0.0:8080\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:30,393 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:30,399 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Connection accepted: /home/model-server/tmp/.mms.sock.9000.\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m Model server started.\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:30,421 [WARN ] pool-2-thread-1 com.amazonaws.ml.mms.metrics.MetricCollector - worker pid is not available yet.\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:32,248 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-0000000e-00000001-e909a2ba47e9ced1-8a702e95\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:32,258 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1768\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:32,262 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-2\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:32,277 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Model model loaded io_fd=0242acfffe120002-0000000e-00000002-4b8d62ba47e9ced1-b0b1cd44\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:32,278 [INFO ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerThread - Backend response time: 1790\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:32,278 [WARN ] W-9000-model com.amazonaws.ml.mms.wlm.WorkerLifeCycle - attachIOStreams() threadName=W-model-1\n",
      "\u001b[36mcrhfmegqhy-algo-1-hwq1z |\u001b[0m 2023-02-19 18:57:34,195 [INFO ] pool-1-thread-4 ACCESS_LOG - /172.18.0.1:45292 \"GET /ping HTTP/1.1\" 200 23\n",
      "!"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "# Deploy the model\n",
    "instance_type = \"local\"\n",
    "\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058a13ca-c377-42b8-9eed-797ea92b3a9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_15811/675315772.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdummy_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"UserID\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_recs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"3\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mdummy_data\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'"
     ]
    }
   ],
   "source": [
    "dummy_data = {\"UserID\": \"5\", \"num_recs\": \"3\"}\n",
    "res = predictor.predict({\"inputs\" : dummy_data})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee6bdf4-f650-4353-8322-15bc82690377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
